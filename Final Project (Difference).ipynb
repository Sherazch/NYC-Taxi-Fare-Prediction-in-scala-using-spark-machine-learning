{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Fare Pridiction  (using Difference between pickup and dropoff points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a training dataset comprised of pick up and drop off locations and we are gonna predict the fare amount for taxi rides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master=\"yarn\"\n",
    "launcher.num_executors=6\n",
    "launcher.executor_cores=2\n",
    "launcher.executor_memory='5000m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://C570BD-HM-Master:8088/proxy/application_1544119661487_0002\n",
       "SparkContext available as 'sc' (version = 2.3.2, master = yarn, app id = application_1544119661487_0002)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-06 12:43:27 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-12-06 12:43:30 WARN  Client:66 - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "training_data: org.apache.spark.sql.DataFrame = [key: timestamp, fare_amount: double ... 6 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val training_data=spark.read.option(\"header\",\"true\").option(\"inferschema\", \"true\").csv(\"/hadoop-user/data/train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see all the attributes in the dataset, their data types and will make new columns using the available ones and then downsample the data because the size of data is too big, i.e. 5.7 GB (55 million records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "|                key|fare_amount|     pickup_datetime|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|\n",
      "+-------------------+-----------+--------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "|2009-06-15 17:26:21|        4.5|2009-06-15 17:26:...|      -73.844311|      40.721319|        -73.84161|       40.712278|              1|\n",
      "|2010-01-05 16:52:16|       16.9|2010-01-05 16:52:...|      -74.016048|      40.711303|       -73.979268|       40.782004|              1|\n",
      "|2011-08-18 00:35:00|        5.7|2011-08-18 00:35:...|      -73.982738|       40.76127|       -73.991242|       40.750562|              2|\n",
      "|2012-04-21 04:30:42|        7.7|2012-04-21 04:30:...|       -73.98713|      40.733143|       -73.991567|       40.758092|              1|\n",
      "|2010-03-09 07:51:00|        5.3|2010-03-09 07:51:...|      -73.968095|      40.768008|       -73.956655|       40.783762|              1|\n",
      "+-------------------+-----------+--------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: timestamp (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//to see the data types of the attributes\n",
    "training_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate the difference between pickup logitude and latitude, and the drop off longitude and latitude and add the new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train1: org.apache.spark.sql.DataFrame = [key: timestamp, fare_amount: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val train1=training_data.withColumn(\"diff_long\",expr((\"dropoff_longitude - pickup_longitude\"))).\n",
    "withColumn(\"diff_lat\",expr((\"dropoff_latitude - pickup_latitude\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train2: org.apache.spark.sql.DataFrame = [key: timestamp, fare_amount: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//taking the absolute values of the difference\n",
    "val train2=train1.withColumn(\"diff_long\",abs(col(\"diff_long\"))).\n",
    "withColumn(\"diff_lat\",abs(col(\"diff_lat\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: timestamp (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- diff_long: double (nullable = true)\n",
      " |-- diff_lat: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res2: Long = 55423856\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train2.printSchema()\n",
    "train2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------------------+----------------+---------------+-----------------+----------------+---------------+--------------------+--------------------+\n",
      "|                key|fare_amount|     pickup_datetime|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|           diff_long|            diff_lat|\n",
      "+-------------------+-----------+--------------------+----------------+---------------+-----------------+----------------+---------------+--------------------+--------------------+\n",
      "|2009-06-15 17:26:21|        4.5|2009-06-15 17:26:...|      -73.844311|      40.721319|        -73.84161|       40.712278|              1|0.002701000000001841|0.009041000000003407|\n",
      "|2010-01-05 16:52:16|       16.9|2010-01-05 16:52:...|      -74.016048|      40.711303|       -73.979268|       40.782004|              1| 0.03677999999999315| 0.07070099999999968|\n",
      "|2011-08-18 00:35:00|        5.7|2011-08-18 00:35:...|      -73.982738|       40.76127|       -73.991242|       40.750562|              2|0.008504000000002065| 0.01070800000000105|\n",
      "|2012-04-21 04:30:42|        7.7|2012-04-21 04:30:...|       -73.98713|      40.733143|       -73.991567|       40.758092|              1|0.004437000000010016|0.024948999999999444|\n",
      "|2010-03-09 07:51:00|        5.3|2010-03-09 07:51:...|      -73.968095|      40.768008|       -73.956655|       40.783762|              1|0.011440000000007444|0.015754000000001156|\n",
      "+-------------------+-----------+--------------------+----------------+---------------+-----------------+----------------+---------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train3: org.apache.spark.sql.DataFrame = [key: timestamp, fare_amount: double ... 8 more fields]\n",
       "res4: Long = 55423480\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// dropping off the null values\n",
    "val train3=train2.na.drop()\n",
    "train3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tr4: org.apache.spark.sql.DataFrame = [key: timestamp, fare_amount: double ... 8 more fields]\n",
       "tr5: org.apache.spark.sql.DataFrame = [fare_amount: double, pickup_longitude: double ... 6 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// dropping of the values of difference that are greator then 5 because they are outliers\n",
    "val tr4=train3.filter($\"diff_long\" < 5).filter($\"diff_lat\" < 5).filter($\"fare_amount\" > 0).toDF()\n",
    "val tr5=tr4.drop(col(\"pickup_datetime\")).drop(col(\"key\")).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "factor: Double = 0.25\n",
       "downSampledData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [fare_amount: double, pickup_longitude: double ... 6 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Downsampling the dataset, we are taking 25% of the data\n",
    "val factor=0.25\n",
    "val downSampledData=tr5.sample(true,factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Long = 13827233\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downSampledData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+---------------+-----------------+----------------+---------------+--------------------+--------------------+\n",
      "|fare_amount|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|           diff_long|            diff_lat|\n",
      "+-----------+----------------+---------------+-----------------+----------------+---------------+--------------------+--------------------+\n",
      "|       16.9|      -74.016048|      40.711303|       -73.979268|       40.782004|              1| 0.03677999999999315| 0.07070099999999968|\n",
      "|       16.5|        -73.9513|      40.774138|       -73.990095|       40.751048|              1| 0.03879499999999325|0.023090000000003386|\n",
      "|        7.0|       -74.00536|      40.728867|       -74.008913|       40.710907|              1|0.003553000000010...|0.017960000000002196|\n",
      "|       11.5|      -73.957954|      40.779252|        -73.96125|       40.758787|              1|0.003296000000005961| 0.02046500000000151|\n",
      "|        4.5|      -73.991707|      40.770505|       -73.985459|       40.763671|              1|0.006247999999999365|0.006833999999997786|\n",
      "+-----------+----------------+---------------+-----------------+----------------+---------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "downSampledData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will separate the target variable and then assemble the features for processing by the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature._\n",
       "numeric_features: Array[String] = Array(pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, passenger_count, diff_long, diff_lat)\n",
       "vectorizer_numeric: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_6d0bd41afafa\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "//get all the numeric features except the target variable\n",
    "val numeric_features=downSampledData.columns.filter(c =>  !c.equals(\"fare_amount\") )\n",
    "\n",
    "//Use VectorAssesmbler to aseemble numeric features into a vector\n",
    "val vectorizer_numeric=new VectorAssembler().setInputCols(numeric_features).setOutputCol(\"features\")\n",
    "\n",
    "//Create an estimator to standardize the numeric feature\n",
    "//val standardizer=new StandardScaler().setWithMean(true).setInputCol(\"numeric_features\").setOutputCol(\"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Linear Regression for fare prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml._\n",
       "import org.apache.spark.ml.feature._\n",
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "import org.apache.spark.ml.regression._\n",
       "LR: org.apache.spark.ml.regression.LinearRegression = linReg_38b43dbfc71e\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml._\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.regression._\n",
    "//Creating the linearRegression model and fit it to the transformed training data\n",
    "val LR= new LinearRegression().setLabelCol(\"fare_amount\").setFeaturesCol(\"features\").\n",
    "setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_b5669d24d1d1\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Creating a Pipeline and add the transformation we did so far to this pipeline\n",
    "val pipeline = new Pipeline().setStages(Array(vectorizer_numeric, LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [fare_amount: double, pickup_longitude: double ... 6 more fields]\n",
       "testing: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [fare_amount: double, pickup_longitude: double ... 6 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Split the data randomly to 80% tranining and 20% testing. The training data is used to build the model and the testing data is used for testing the model\n",
    "val Array(training,testing)=downSampledData.randomSplit(Array(0.8,0.2),111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-06 12:55:05 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2018-12-06 12:55:05 WARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation._\n",
       "pipeline_model: org.apache.spark.ml.PipelineModel = pipeline_b5669d24d1d1\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation._\n",
    "\n",
    "//Fitting the pipeline to the traning data and transforming the training data\n",
    "val pipeline_model= pipeline.fit(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+--------------------+\n",
      "|       prediction|fare_amount|            features|\n",
      "+-----------------+-----------+--------------------+\n",
      "|6.580124890951362|        2.5|[-74.115267,40.66...|\n",
      "|6.590436438975953|        2.5|[-74.037935,40.71...|\n",
      "|6.581130692465249|        2.5|[-74.036437,40.75...|\n",
      "|6.582617250543411|        2.5|[-74.034508,40.72...|\n",
      "|12.45539479985031|        2.5|[-74.01578,40.705...|\n",
      "+-----------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation._\n",
       "predictions: org.apache.spark.sql.DataFrame = [fare_amount: double, pickup_longitude: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation._\n",
    "\n",
    "//apllyintg the model to the test data to make predictions\n",
    "val predictions = pipeline_model.transform(testing)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"prediction\",\"fare_amount\", \"features\").show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new RegressionEvaluator()\n",
    "  .setLabelCol(\"fare_amount\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"rmse\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBT Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.{GBTRegressionModel, GBTRegressor}\n",
       "gbt: org.apache.spark.ml.regression.GBTRegressor = gbtr_3a892327b813\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.{GBTRegressionModel, GBTRegressor}\n",
    "\n",
    "// Create a GBT model.\n",
    "val gbt = new GBTRegressor()\n",
    "  .setLabelCol(\"fare_amount\")\n",
    "  .setFeaturesCol(\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_gbt: org.apache.spark.ml.Pipeline = pipeline_7025db886c8b\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_gbt = new Pipeline().setStages(Array(vectorizer_numeric, gbt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_gbt: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [fare_amount: double, pickup_longitude: double ... 6 more fields]\n",
       "testing_gbt: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [fare_amount: double, pickup_longitude: double ... 6 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Split the data randomly to 80% tranining and 20% testing. The training data is used to build the model and the testing data is used for testing the model\n",
    "val Array(training_gbt,testing_gbt)=downSampledData.randomSplit(Array(0.8,0.2),111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_0 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_23 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_7 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_39 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_39 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_32 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_8 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_12 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_36 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_32 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_27 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_1 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_0 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_24 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_15 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_19 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_32 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_27 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_24 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_19 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_35 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_36 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_8 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_12 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_19 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_39 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_36 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_20 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_3 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_11 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_28 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_23 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_12 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_31 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_0 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_1 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_8 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_27 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_20 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_32 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_11 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_3 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_24 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_23 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_28 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_8 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_42 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_11 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_35 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_35 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_11 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_15 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_31 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_20 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_27 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_7 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_16 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_3 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_42 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_15 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_31 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_16 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_7 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_12 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_39 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_15 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_1 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_7 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_20 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_3 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_23 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_42 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_16 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_35 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_28 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_36 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_28 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_42 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_963_16 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_1 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_31 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_990_24 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_936_19 !\n",
      "2018-12-06 13:58:14 WARN  BlockManagerMasterEndpoint:66 - No more replicas available for rdd_722_0 !\n",
      "2018-12-06 13:58:14 ERROR YarnScheduler:70 - Lost executor 3 on CSC570BD-S1: Container killed by YARN for exceeding memory limits. 5.4 GB of 5.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-12-06 13:58:14 WARN  TaskSetManager:66 - Lost task 1.0 in stage 482.0 (TID 17353, CSC570BD-S1, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 5.4 GB of 5.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-12-06 13:58:14 WARN  TaskSetManager:66 - Lost task 0.0 in stage 482.0 (TID 17351, CSC570BD-S1, executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 5.4 GB of 5.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n",
      "2018-12-06 13:58:14 WARN  YarnSchedulerBackend$YarnSchedulerEndpoint:66 - Requesting driver to remove executor 3 for reason Container killed by YARN for exceeding memory limits. 5.4 GB of 5.4 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pipeline_model_gbt: org.apache.spark.ml.PipelineModel = pipeline_7025db886c8b\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_model_gbt= pipeline_gbt.fit(training_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+--------------------+\n",
      "|        prediction|fare_amount|            features|\n",
      "+------------------+-----------+--------------------+\n",
      "| 48.22619519179591|        2.5|[-74.115267,40.66...|\n",
      "|25.369613964560102|        2.5|[-74.037935,40.71...|\n",
      "| 36.73248646834585|        2.5|[-74.036437,40.75...|\n",
      "| 34.95496567378947|        2.5|[-74.034508,40.72...|\n",
      "|13.838395376308867|        2.5|[-74.01578,40.705...|\n",
      "+------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions_gbt: org.apache.spark.sql.DataFrame = [fare_amount: double, pickup_longitude: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//applyintg the model to the test data to make predictions\n",
    "val predictions_gbt = pipeline_model_gbt.transform(testing_gbt)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions_gbt.select(\"prediction\", \"fare_amount\",\"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 4.795093561075967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator_gbt: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_c610a4ae0347\n",
       "rmse_gbt: Double = 4.795093561075967\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator_gbt = new RegressionEvaluator()\n",
    "  .setLabelCol(\"fare_amount\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"rmse\")\n",
    "val rmse_gbt = evaluator_gbt.evaluate(predictions_gbt)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse_gbt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}\n",
       "RF: org.apache.spark.ml.regression.RandomForestRegressor = rfr_72ccd2af2ddb\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}\n",
    "\n",
    "// Create a RF Regression model.\n",
    "val RF = new RandomForestRegressor()\n",
    "  .setLabelCol(\"fare_amount\")\n",
    "  .setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_rf: org.apache.spark.ml.Pipeline = pipeline_fa08ed7b80d9\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_rf = new Pipeline().setStages(Array(vectorizer_numeric, RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_rf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [fare_amount: double, pickup_longitude: double ... 6 more fields]\n",
       "testing_rf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [fare_amount: double, pickup_longitude: double ... 6 more fields]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Split the data randomly to 80% tranining and 20% testing. The training data is used to build the model and the testing data is used for testing the model\n",
    "val Array(training_rf,testing_rf)=downSampledData.randomSplit(Array(0.8,0.2),111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_model_rf: org.apache.spark.ml.PipelineModel = pipeline_fa08ed7b80d9\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_model_rf= pipeline_rf.fit(training_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+--------------------+\n",
      "|        prediction|fare_amount|            features|\n",
      "+------------------+-----------+--------------------+\n",
      "|10.148876263017446|        2.5|[-74.115267,40.66...|\n",
      "|  7.97000768220021|        2.5|[-74.037935,40.71...|\n",
      "| 10.33006375060903|        2.5|[-74.036437,40.75...|\n",
      "| 8.337633665873257|        2.5|[-74.034508,40.72...|\n",
      "|12.746673314664807|        2.5|[-74.01578,40.705...|\n",
      "+------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions_rf: org.apache.spark.sql.DataFrame = [fare_amount: double, pickup_longitude: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//apllyintg the model to the test data to make predictions\n",
    "val predictions_rf = pipeline_model_rf.transform(testing_rf)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions_rf.select(\"prediction\", \"fare_amount\",\"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 5.139040224281306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator_rf: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_57ecea0e0c04\n",
       "rmse_rf: Double = 5.139040224281306\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator_rf = new RegressionEvaluator()\n",
    "  .setLabelCol(\"fare_amount\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"rmse\")\n",
    "val rmse_rf = evaluator_rf.evaluate(predictions_rf)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse_rf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.{DecisionTreeRegressionModel, DecisionTreeRegressor}\n",
       "DT: org.apache.spark.ml.regression.DecisionTreeRegressor = dtr_9928a8f912df\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.{DecisionTreeRegressionModel, DecisionTreeRegressor}\n",
    "\n",
    "// Create a Decision Tree Regression model.\n",
    "val DT = new DecisionTreeRegressor()\n",
    "  .setLabelCol(\"fare_amount\")\n",
    "  .setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_dt: org.apache.spark.ml.Pipeline = pipeline_b8ae04e1fbd3\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_dt = new Pipeline().setStages(Array(vectorizer_numeric, DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_dt: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [fare_amount: double, pickup_longitude: double ... 6 more fields]\n",
       "testing_dt: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [fare_amount: double, pickup_longitude: double ... 6 more fields]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Split the data randomly to 80% tranining and 20% testing. The training data is used to build the model and the testing data is used for testing the model\n",
    "val Array(training_dt,testing_dt)=downSampledData.randomSplit(Array(0.8,0.2),111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_model_dt: org.apache.spark.ml.PipelineModel = pipeline_b8ae04e1fbd3\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_model_dt= pipeline_dt.fit(training_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+--------------------+\n",
      "|        prediction|fare_amount|            features|\n",
      "+------------------+-----------+--------------------+\n",
      "|11.961113633244354|        2.5|[-74.115267,40.66...|\n",
      "|11.961113633244354|        2.5|[-74.037935,40.71...|\n",
      "|11.961113633244354|        2.5|[-74.036437,40.75...|\n",
      "|11.961113633244354|        2.5|[-74.034508,40.72...|\n",
      "|13.151258649816727|        2.5|[-74.01578,40.705...|\n",
      "+------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions_dt: org.apache.spark.sql.DataFrame = [fare_amount: double, pickup_longitude: double ... 8 more fields]\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//apllyintg the model to the test data to make predictions\n",
    "val predictions_dt = pipeline_model_dt.transform(testing_dt)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions_dt.select(\"prediction\",\"fare_amount\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 5.125889477501701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator_dt: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_e1ddc52ffb2c\n",
       "rmse_dt: Double = 5.125889477501701\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator_dt = new RegressionEvaluator()\n",
    "  .setLabelCol(\"fare_amount\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"rmse\")\n",
    "val rmse_dt = evaluator_dt.evaluate(predictions_dt)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse_dt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By seeing the RMSE of all the regression alogrithms, we can say that GBT is the most efficient because the RMSE value is the lowest in that, i.e. 4.79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
