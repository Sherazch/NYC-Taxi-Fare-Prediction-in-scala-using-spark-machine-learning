{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Fare Prediction (Using total distance in Kms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a training dataset comprised of pick up and drop off locations and we are gonna predict the fare amount for taxi rides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master=\"yarn\"\n",
    "launcher.num_executors=6\n",
    "launcher.executor_cores=2\n",
    "launcher.executor_memory='2500m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://HM11:8088/proxy/application_1544141973356_0015\n",
       "SparkContext available as 'sc' (version = 2.3.2, master = yarn, app id = application_1544141973356_0015)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-07 00:31:08 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2018-12-07 00:31:11 WARN  Client:66 - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "training_data: org.apache.spark.sql.DataFrame = [key: timestamp, fare_amount: double ... 6 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val training_data=spark.read.option(\"header\",\"true\").option(\"inferschema\", \"true\").csv(\"/project/train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see all the attributes in the dataset, their data types and will make new columns using the available ones and then downsample the data because the size of data is too big, i.e. 5.7 GB (55 million records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "|                key|fare_amount|     pickup_datetime|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|\n",
      "+-------------------+-----------+--------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "|2009-06-15 17:26:21|        4.5|2009-06-15 17:26:...|      -73.844311|      40.721319|        -73.84161|       40.712278|              1|\n",
      "|2010-01-05 16:52:16|       16.9|2010-01-05 16:52:...|      -74.016048|      40.711303|       -73.979268|       40.782004|              1|\n",
      "|2011-08-18 00:35:00|        5.7|2011-08-18 00:35:...|      -73.982738|       40.76127|       -73.991242|       40.750562|              2|\n",
      "|2012-04-21 04:30:42|        7.7|2012-04-21 04:30:...|       -73.98713|      40.733143|       -73.991567|       40.758092|              1|\n",
      "|2010-03-09 07:51:00|        5.3|2010-03-09 07:51:...|      -73.968095|      40.768008|       -73.956655|       40.783762|              1|\n",
      "+-------------------+-----------+--------------------+----------------+---------------+-----------------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: timestamp (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//to see the data types of the attributes\n",
    "training_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are gonna calculate the distance in Kms by using formula below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temp: org.apache.spark.sql.DataFrame = [key: timestamp, fare_amount: double ... 8 more fields]\n",
       "temp1: org.apache.spark.sql.DataFrame = [key: timestamp, fare_amount: double ... 7 more fields]\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val temp = training_data.withColumn(\"a\", pow(sin(toRadians($\"dropoff_latitude\" - $\"pickup_latitude\") / 2), 2) + cos(toRadians($\"pickup_latitude\")) * cos(toRadians($\"dropoff_latitude\")) * pow(sin(toRadians($\"dropoff_longitude\" - $\"pickup_longitude\") / 2), 2)).withColumn(\"distance\", atan2(sqrt($\"a\"), sqrt(-$\"a\" + 1)) * 2 * 6371)\n",
    "val temp1 = temp.drop(col(\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+--------------------+----------------+---------------+-----------------+----------------+---------------+------------------+\n",
      "|                key|fare_amount|     pickup_datetime|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|          distance|\n",
      "+-------------------+-----------+--------------------+----------------+---------------+-----------------+----------------+---------------+------------------+\n",
      "|2009-06-15 17:26:21|        4.5|2009-06-15 17:26:...|      -73.844311|      40.721319|        -73.84161|       40.712278|              1|1.0307639350492535|\n",
      "|2010-01-05 16:52:16|       16.9|2010-01-05 16:52:...|      -74.016048|      40.711303|       -73.979268|       40.782004|              1| 8.450133595806088|\n",
      "|2011-08-18 00:35:00|        5.7|2011-08-18 00:35:...|      -73.982738|       40.76127|       -73.991242|       40.750562|              2|1.3895252257699269|\n",
      "|2012-04-21 04:30:42|        7.7|2012-04-21 04:30:...|       -73.98713|      40.733143|       -73.991567|       40.758092|              1|2.7992702399835117|\n",
      "|2010-03-09 07:51:00|        5.3|2010-03-09 07:51:...|      -73.968095|      40.768008|       -73.956655|       40.783762|              1|1.9991567879961665|\n",
      "+-------------------+-----------+--------------------+----------------+---------------+-----------------+----------------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the null values from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train3: org.apache.spark.sql.DataFrame = [key: timestamp, fare_amount: double ... 7 more fields]\n",
       "res22: Long = 55423480\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val train3=temp1.na.drop()\n",
    "train3.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are gonna filter the rows in which distance is greator then 35 or in which fare amount is greator then 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tr4: org.apache.spark.sql.DataFrame = [key: timestamp, fare_amount: double ... 7 more fields]\n",
       "tr5: org.apache.spark.sql.DataFrame = [fare_amount: double, pickup_longitude: double ... 5 more fields]\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tr4=train3.filter($\"distance\" < 35).filter($\"fare_amount\" > 0).toDF()\n",
    "val tr5=tr4.drop(col(\"pickup_datetime\")).drop(col(\"key\")).drop().toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsampling the dataset, we are taking only 25% of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "factor: Double = 0.25\n",
       "downSampledData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [fare_amount: double, pickup_longitude: double ... 5 more fields]\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val factor=0.25\n",
    "val downSampledData=tr5.sample(true,factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res25: Long = 13819780\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downSampledData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+---------------+-----------------+----------------+---------------+------------------+\n",
      "|fare_amount|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|passenger_count|          distance|\n",
      "+-----------+----------------+---------------+-----------------+----------------+---------------+------------------+\n",
      "|        4.5|      -73.844311|      40.721319|        -73.84161|       40.712278|              1|1.0307639350492535|\n",
      "|        5.3|      -73.968095|      40.768008|       -73.956655|       40.783762|              1|1.9991567879961665|\n",
      "|       16.5|        -73.9513|      40.774138|       -73.990095|       40.751048|              1| 4.155444291845964|\n",
      "|        9.0|      -74.006462|      40.726713|       -73.993078|       40.731628|              1| 1.253231512725611|\n",
      "|       10.5|      -73.985382|      40.747858|       -73.978377|        40.76207|              1| 1.686861330169933|\n",
      "+-----------+----------------+---------------+-----------------+----------------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "downSampledData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will separate the target variable and then assemble the features for processing by the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature._\n",
       "numeric_features: Array[String] = Array(pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, passenger_count, distance)\n",
       "vectorizer_numeric: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_bf28be0dcfae\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "//get all the numeric features except the target variable\n",
    "val numeric_features=downSampledData.columns.filter(c =>  !c.equals(\"fare_amount\") )\n",
    "\n",
    "\n",
    "//Use VectorAssesmbler to aseemble numeric features into a vector\n",
    "val vectorizer_numeric=new VectorAssembler().setInputCols(numeric_features).setOutputCol(\"features\")\n",
    "\n",
    "//Create an estimator to standardize the numeric feature\n",
    "//val standardizer=new StandardScaler().setWithMean(true).setInputCol(\"numeric_features\").setOutputCol(\"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Linear Regression for fare prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml._\n",
       "import org.apache.spark.ml.feature._\n",
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "import org.apache.spark.ml.regression._\n",
       "LR: org.apache.spark.ml.regression.LinearRegression = linReg_4a02d3bb9414\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml._\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.regression._\n",
    "//Creating the linearRegression model and fit it to the transformed training data\n",
    "val LR= new LinearRegression().setLabelCol(\"fare_amount\").setFeaturesCol(\"features\").\n",
    "setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_de8d6aeecd30\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Creating a Pipeline and add the transformation we did so far to this pipeline\n",
    "val pipeline = new Pipeline().setStages(Array(vectorizer_numeric, LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [fare_amount: double, pickup_longitude: double ... 5 more fields]\n",
       "testing: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [fare_amount: double, pickup_longitude: double ... 5 more fields]\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Split the data randomly to 80% tranining and 20% testing. The training data is used to build the model and the testing data is used for testing the model\n",
    "val Array(training,testing)=downSampledData.randomSplit(Array(0.8,0.2),111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation._\n",
       "pipeline_model: org.apache.spark.ml.PipelineModel = pipeline_de8d6aeecd30\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation._\n",
    "\n",
    "//Fitting the pipeline to the traning data and transforming the training data\n",
    "val pipeline_model= pipeline.fit(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+--------------------+\n",
      "|        prediction|fare_amount|            features|\n",
      "+------------------+-----------+--------------------+\n",
      "| 3.986536047272377|        2.5|[-74.254295,40.67...|\n",
      "| 4.000030862058741|        2.5|[-74.029732,40.75...|\n",
      "|4.0132666761122975|        2.5|[-74.024305,40.60...|\n",
      "|4.0132666761122975|        2.5|[-74.024305,40.60...|\n",
      "| 4.107659212920624|        2.5|[-74.015233,40.71...|\n",
      "+------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation._\n",
       "predictions: org.apache.spark.sql.DataFrame = [fare_amount: double, pickup_longitude: double ... 7 more fields]\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation._\n",
    "\n",
    "//apllyintg the model to the test data to make predictions\n",
    "val predictions = pipeline_model.transform(testing)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"prediction\",\"fare_amount\", \"features\").show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 5.164927129414834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_956e76753b35\n",
       "rmse: Double = 5.164927129414834\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new RegressionEvaluator()\n",
    "  .setLabelCol(\"fare_amount\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"rmse\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBT Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.{GBTRegressionModel, GBTRegressor}\n",
       "gbt: org.apache.spark.ml.regression.GBTRegressor = gbtr_e29f8f3fdecc\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.{GBTRegressionModel, GBTRegressor}\n",
    "\n",
    "// Create a GBT model.\n",
    "val gbt = new GBTRegressor()\n",
    "  .setLabelCol(\"fare_amount\")\n",
    "  .setFeaturesCol(\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_gbt: org.apache.spark.ml.Pipeline = pipeline_39f3229a4a8d\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_gbt = new Pipeline().setStages(Array(vectorizer_numeric, gbt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_gbt: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [fare_amount: double, pickup_longitude: double ... 5 more fields]\n",
       "testing_gbt: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [fare_amount: double, pickup_longitude: double ... 5 more fields]\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Split the data randomly to 80% tranining and 20% testing. The training data is used to build the model and the testing data is used for testing the model\n",
    "val Array(training_gbt,testing_gbt)=downSampledData.randomSplit(Array(0.8,0.2),111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_model_gbt: org.apache.spark.ml.PipelineModel = pipeline_39f3229a4a8d\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_model_gbt= pipeline_gbt.fit(training_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+--------------------+\n",
      "|        prediction|fare_amount|            features|\n",
      "+------------------+-----------+--------------------+\n",
      "|38.155541048769095|        2.5|[-74.254295,40.67...|\n",
      "|32.911417927072534|        2.5|[-74.029732,40.75...|\n",
      "| 34.66767129887482|        2.5|[-74.024305,40.60...|\n",
      "| 34.66767129887482|        2.5|[-74.024305,40.60...|\n",
      "| 7.742086164449066|        2.5|[-74.015233,40.71...|\n",
      "+------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions_gbt: org.apache.spark.sql.DataFrame = [fare_amount: double, pickup_longitude: double ... 7 more fields]\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//applyintg the model to the test data to make predictions\n",
    "val predictions_gbt = pipeline_model_gbt.transform(testing_gbt)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions_gbt.select(\"prediction\", \"fare_amount\",\"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 4.697264054494451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator_gbt: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_33dce0337877\n",
       "rmse_gbt: Double = 4.697264054494451\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator_gbt = new RegressionEvaluator()\n",
    "  .setLabelCol(\"fare_amount\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"rmse\")\n",
    "val rmse_gbt = evaluator_gbt.evaluate(predictions_gbt)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse_gbt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}\n",
       "RF: org.apache.spark.ml.regression.RandomForestRegressor = rfr_c4061cb49359\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}\n",
    "\n",
    "// Create a RF Regression model.\n",
    "val RF = new RandomForestRegressor()\n",
    "  .setLabelCol(\"fare_amount\")\n",
    "  .setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_rf: org.apache.spark.ml.Pipeline = pipeline_2e22137c602b\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_rf = new Pipeline().setStages(Array(vectorizer_numeric, RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_rf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [fare_amount: double, pickup_longitude: double ... 5 more fields]\n",
       "testing_rf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [fare_amount: double, pickup_longitude: double ... 5 more fields]\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Split the data randomly to 80% tranining and 20% testing. The training data is used to build the model and the testing data is used for testing the model\n",
    "val Array(training_rf,testing_rf)=downSampledData.randomSplit(Array(0.8,0.2),111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_model_rf: org.apache.spark.ml.PipelineModel = pipeline_2e22137c602b\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_model_rf= pipeline_rf.fit(training_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+--------------------+\n",
      "|        prediction|fare_amount|            features|\n",
      "+------------------+-----------+--------------------+\n",
      "|16.438061360259272|        2.5|[-74.254295,40.67...|\n",
      "|13.962456553318157|        2.5|[-74.029732,40.75...|\n",
      "|17.422949328601113|        2.5|[-74.024305,40.60...|\n",
      "|17.422949328601113|        2.5|[-74.024305,40.60...|\n",
      "| 9.085559578182544|        2.5|[-74.015233,40.71...|\n",
      "+------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions_rf: org.apache.spark.sql.DataFrame = [fare_amount: double, pickup_longitude: double ... 7 more fields]\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//apllyintg the model to the test data to make predictions\n",
    "val predictions_rf = pipeline_model_rf.transform(testing_rf)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions_rf.select(\"prediction\", \"fare_amount\",\"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 5.086012994281541"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator_rf: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_e4064fb8021a\n",
       "rmse_rf: Double = 5.086012994281541\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator_rf = new RegressionEvaluator()\n",
    "  .setLabelCol(\"fare_amount\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"rmse\")\n",
    "val rmse_rf = evaluator_rf.evaluate(predictions_rf)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse_rf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.{DecisionTreeRegressionModel, DecisionTreeRegressor}\n",
       "DT: org.apache.spark.ml.regression.DecisionTreeRegressor = dtr_a6d474e37f38\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.{DecisionTreeRegressionModel, DecisionTreeRegressor}\n",
    "\n",
    "// Create a Decision Tree Regression model.\n",
    "val DT = new DecisionTreeRegressor()\n",
    "  .setLabelCol(\"fare_amount\")\n",
    "  .setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_dt: org.apache.spark.ml.Pipeline = pipeline_c4f4b278378c\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_dt = new Pipeline().setStages(Array(vectorizer_numeric, DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_dt: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [fare_amount: double, pickup_longitude: double ... 5 more fields]\n",
       "testing_dt: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [fare_amount: double, pickup_longitude: double ... 5 more fields]\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Split the data randomly to 80% tranining and 20% testing. The training data is used to build the model and the testing data is used for testing the model\n",
    "val Array(training_dt,testing_dt)=downSampledData.randomSplit(Array(0.8,0.2),111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_model_dt: org.apache.spark.ml.PipelineModel = pipeline_c4f4b278378c\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_model_dt= pipeline_dt.fit(training_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+--------------------+\n",
      "|        prediction|fare_amount|            features|\n",
      "+------------------+-----------+--------------------+\n",
      "|29.040809586264455|        2.5|[-74.254295,40.67...|\n",
      "|29.040809586264455|        2.5|[-74.029732,40.75...|\n",
      "| 5.736257392009186|        2.5|[-74.024305,40.60...|\n",
      "| 5.736257392009186|        2.5|[-74.024305,40.60...|\n",
      "| 5.736257392009186|        2.5|[-74.015233,40.71...|\n",
      "+------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions_dt: org.apache.spark.sql.DataFrame = [fare_amount: double, pickup_longitude: double ... 7 more fields]\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//apllyintg the model to the test data to make predictions\n",
    "val predictions_dt = pipeline_model_dt.transform(testing_dt)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions_dt.select(\"prediction\",\"fare_amount\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 4.955962326461401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator_dt: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_3f441d548833\n",
       "rmse_dt: Double = 4.955962326461401\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator_dt = new RegressionEvaluator()\n",
    "  .setLabelCol(\"fare_amount\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"rmse\")\n",
    "val rmse_dt = evaluator_dt.evaluate(predictions_dt)\n",
    "println(s\"Root Mean Squared Error (RMSE) on test data = $rmse_dt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By seeing the RMSE values, we can say that the GBT model is better because it has lowest RMSE, i.e. 4.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
